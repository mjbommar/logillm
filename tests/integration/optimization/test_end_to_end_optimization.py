"""End-to-end integration tests for optimization workflows.

These tests use real APIs to validate that optimization actually improves performance.
NO MOCKS - these test the full optimization pipeline with real LLM calls.
"""

from typing import Any

import pytest

from logillm.core.predict import Predict
from logillm.optimizers.bootstrap_fewshot import BootstrapFewShot
from logillm.optimizers.format_optimizer import FormatOptimizer
from logillm.optimizers.hybrid_optimizer import HybridOptimizer
from logillm.optimizers.hyperparameter import HyperparameterOptimizer


@pytest.mark.integration
@pytest.mark.openai
class TestHyperparameterOptimizationE2E:
    """Test hyperparameter optimization end-to-end."""

    @pytest.mark.asyncio
    async def test_temperature_optimization_improves_accuracy(
        self, openai_provider_registered, simple_qa_dataset
    ):
        """Test that optimizing temperature actually improves accuracy."""
        # Create a simple QA module
        qa_module = Predict("question -> answer")

        # Define a strict accuracy metric
        def exact_match_metric(pred_outputs: dict[str, Any], true_outputs: dict[str, Any]) -> float:
            """Exact match metric for Q&A."""
            pred = str(pred_outputs.get("answer", "")).lower().strip()
            true = str(true_outputs.get("answer", "")).lower().strip()
            return 1.0 if pred == true else 0.0

        # Test baseline performance with default settings
        baseline_scores = []
        for example in simple_qa_dataset:
            result = await qa_module.forward(**example["inputs"])
            if result.success:
                score = exact_match_metric(result.outputs, example["outputs"])
                baseline_scores.append(score)

        baseline_avg = sum(baseline_scores) / len(baseline_scores) if baseline_scores else 0.0

        # Create hyperparameter optimizer
        optimizer = HyperparameterOptimizer(
            metric=exact_match_metric,
            strategy="random",
            n_trials=5,  # Small number for testing
        )

        # Optimize
        result = await optimizer.optimize(
            module=qa_module,
            trainset=simple_qa_dataset[:2],  # Small training set
            valset=simple_qa_dataset[2:],  # Small validation set
        )

        # Verify optimization completed
        assert result.optimized_module is not None
        assert result.best_score >= 0.0
        assert "best_config" in result.metadata
        # Check that optimization history exists (might be named "history" not "score_history")
        history_key = "score_history" if "score_history" in result.metadata else "history"
        assert history_key in result.metadata
        if result.metadata[history_key]:
            assert len(result.metadata[history_key]) > 0

        # Test optimized performance
        optimized_module = result.optimized_module
        optimized_scores = []
        for example in simple_qa_dataset:
            opt_result = await optimized_module.forward(**example["inputs"])
            if opt_result.success:
                score = exact_match_metric(opt_result.outputs, example["outputs"])
                optimized_scores.append(score)

        optimized_avg = sum(optimized_scores) / len(optimized_scores) if optimized_scores else 0.0

        # Optimized should be at least as good as baseline
        assert optimized_avg >= baseline_avg - 0.1  # Allow small tolerance

        # Should have found best configuration
        best_config = result.metadata["best_config"]
        assert "temperature" in best_config or "top_p" in best_config


@pytest.mark.integration
@pytest.mark.openai
class TestBootstrapOptimizationE2E:
    """Test bootstrap few-shot optimization end-to-end."""

    @pytest.mark.asyncio
    async def test_bootstrap_improves_with_demos(self, openai_provider_registered, math_dataset):
        """Test that bootstrap few-shot actually improves with demonstrations."""
        # Create math module
        math_module = Predict("x: int, y: int -> result: int")

        # Define accuracy metric for math
        def math_accuracy(pred_outputs: dict[str, Any], true_outputs: dict[str, Any]) -> float:
            """Math accuracy metric."""
            try:
                pred = int(pred_outputs.get("result", 0))
                true = int(true_outputs.get("result", 0))
                return 1.0 if pred == true else 0.0
            except (ValueError, TypeError):
                return 0.0

        # Test baseline performance (no demos)
        baseline_score = 0.0
        for example in math_dataset:
            result = await math_module.forward(**example["inputs"])
            if result.success:
                score = math_accuracy(result.outputs, example["outputs"])
                baseline_score += score
        baseline_score /= len(math_dataset)

        # Create bootstrap optimizer
        optimizer = BootstrapFewShot(
            metric=math_accuracy,
            max_bootstrapped_demos=2,
            metric_threshold=0.3,  # Small number for testing
        )

        # Optimize with bootstrap
        result = await optimizer.optimize(module=math_module, dataset=math_dataset)

        # Verify optimization
        assert result.optimized_module is not None
        assert hasattr(result.optimized_module, "demo_manager")

        # Should have added some demonstrations
        demos = result.optimized_module.demo_manager.demos
        assert len(demos) <= 2  # Max demos as configured

        # Test optimized performance
        optimized_score = 0.0
        for example in math_dataset:
            opt_result = await result.optimized_module.forward(**example["inputs"])
            if opt_result.success:
                score = math_accuracy(opt_result.outputs, example["outputs"])
                optimized_score += score
        optimized_score /= len(math_dataset)

        # Should improve or at least not drastically hurt performance
        # Note: With small datasets and randomness, bootstrap might not always improve
        assert optimized_score >= baseline_score - 0.4  # Allow for some degradation

        # More importantly, check that the optimization process completed successfully
        assert len(demos) >= 0  # At least tried to add demos
        print(
            f"Bootstrap optimization: baseline={baseline_score:.2f}, optimized={optimized_score:.2f}"
        )
        if optimized_score >= baseline_score:
            print("âœ“ Optimization improved performance!")
        else:
            print("! Optimization didn't improve (common with small datasets)")


@pytest.mark.integration
@pytest.mark.openai
class TestHybridOptimizationE2E:
    """Test hybrid optimization (LogiLLM's killer feature) end-to-end."""

    @pytest.mark.asyncio
    @pytest.mark.timeout(300)  # 5 minutes for optimization
    @pytest.mark.slow
    async def test_hybrid_optimization_outperforms_individual(
        self, openai_provider_registered, simple_qa_dataset
    ):
        """Test that hybrid optimization outperforms individual optimizers."""
        # Create QA module
        qa_module = Predict("question -> answer")

        # Simple accuracy metric
        def accuracy_metric(pred_outputs: dict[str, Any], true_outputs: dict[str, Any]) -> float:
            pred = str(pred_outputs.get("answer", "")).lower().strip()
            true = str(true_outputs.get("answer", "")).lower().strip()
            # More lenient matching for integration test
            return 1.0 if true in pred or pred in true else 0.0

        # Test hyperparameter-only optimization
        hyper_optimizer = HyperparameterOptimizer(
            metric=accuracy_metric, strategy="random", n_trials=3
        )

        hyper_result = await hyper_optimizer.optimize(
            module=qa_module, trainset=simple_qa_dataset[:1], valset=simple_qa_dataset[1:2]
        )

        # Test hybrid optimization
        from logillm.optimizers.optimizer_config import HybridOptimizerConfig

        config = HybridOptimizerConfig(
            num_iterations=1,
            n_trials=2,  # Minimal trials for testing
            n_warmup_joint=1,
            demo_subset_size=2,
        )

        hybrid_optimizer = HybridOptimizer(
            metric=accuracy_metric,
            strategy="alternating",
            config=config,  # Reduced for faster testing
        )

        hybrid_result = await hybrid_optimizer.optimize(
            module=qa_module, dataset=simple_qa_dataset[:2], validation_set=simple_qa_dataset[2:]
        )

        # Verify both completed
        assert hyper_result.optimized_module is not None
        assert hybrid_result.optimized_module is not None

        # Hybrid should optimize both prompts and hyperparameters
        hybrid_module = hybrid_result.optimized_module
        assert hasattr(hybrid_module, "config")  # Has hyperparameters
        assert "demonstrations" in hybrid_module.parameters  # Has prompt optimization

        # Both should show improvement or stable performance
        assert hyper_result.improvement >= -0.1  # Allow small tolerance
        assert hybrid_result.improvement >= -0.1

        # Hybrid metadata should show both types of optimization
        assert hybrid_result.metadata["strategy"] == "alternating"
        assert "num_iterations" in hybrid_result.metadata


@pytest.mark.integration
@pytest.mark.openai
class TestFormatOptimizationE2E:
    """Test format optimization end-to-end."""

    @pytest.mark.asyncio
    async def test_format_optimization_finds_best_format(self, openai_provider_registered):
        """Test that format optimization finds the best prompt format."""
        # Create a classification module
        classifier = Predict("text -> category")

        # Simple dataset for format testing
        format_dataset = [
            {"inputs": {"text": "I love this product!"}, "outputs": {"category": "positive"}},
            {"inputs": {"text": "This is terrible."}, "outputs": {"category": "negative"}},
        ]

        # Define metric
        def category_match(pred_outputs: dict[str, Any], true_outputs: dict[str, Any]) -> float:
            pred = str(pred_outputs.get("category", "")).lower()
            true = str(true_outputs.get("category", "")).lower()
            return 1.0 if true in pred else 0.0

        # Create format optimizer - test only one format for speed
        from logillm.optimizers.format_optimizer import FormatOptimizerConfig, PromptFormat

        config = FormatOptimizerConfig(
            formats_to_test=[PromptFormat.JSON],  # Only test JSON for quick tests
            min_samples_per_format=1,
            max_samples_per_format=1,
        )
        optimizer = FormatOptimizer(metric=category_match, config=config, track_by_model=True)

        # Optimize formats
        result = await optimizer.optimize(module=classifier, dataset=format_dataset)

        # Verify optimization
        assert result.optimized_module is not None
        assert "best_format" in result.metadata
        assert "format_scores" in result.metadata

        # Should have tested at least one format
        format_scores = result.metadata["format_scores"]
        assert len(format_scores) >= 1  # Should test at least one format

        # Best format should be one of the known formats
        best_format = result.metadata["best_format"]
        assert best_format in ["markdown", "json", "xml", "hybrid"]


@pytest.mark.integration
@pytest.mark.openai
@pytest.mark.slow
class TestFullOptimizationWorkflow:
    """Test complete optimization workflow from start to finish."""

    @pytest.mark.asyncio
    async def test_complete_optimization_pipeline(self, openai_provider_registered):
        """Test a complete optimization pipeline combining multiple optimizers."""
        # Create a sentiment analysis task
        sentiment_module = Predict("review -> sentiment, confidence")

        # Dataset with clear positive/negative examples
        sentiment_dataset = [
            {
                "inputs": {"review": "This movie is absolutely amazing! Best film ever!"},
                "outputs": {"sentiment": "positive", "confidence": "high"},
            },
            {
                "inputs": {"review": "Terrible movie. Waste of time."},
                "outputs": {"sentiment": "negative", "confidence": "high"},
            },
            {
                "inputs": {"review": "The movie was okay, nothing special."},
                "outputs": {"sentiment": "neutral", "confidence": "medium"},
            },
        ]

        # Multi-criteria metric
        def sentiment_quality(pred_outputs: dict[str, Any], true_outputs: dict[str, Any]) -> float:
            """Evaluate both sentiment accuracy and confidence appropriateness."""
            pred_sentiment = str(pred_outputs.get("sentiment", "")).lower()
            true_sentiment = str(true_outputs.get("sentiment", "")).lower()

            # Sentiment accuracy (main score)
            sentiment_score = 1.0 if true_sentiment in pred_sentiment else 0.0

            # Confidence appropriateness (bonus)
            pred_conf = str(pred_outputs.get("confidence", "")).lower()
            true_conf = str(true_outputs.get("confidence", "")).lower()
            conf_bonus = 0.2 if true_conf in pred_conf else 0.0

            return sentiment_score + conf_bonus

        # Step 1: Baseline performance
        baseline_scores = []
        for example in sentiment_dataset:
            result = await sentiment_module.forward(**example["inputs"])
            if result.success:
                score = sentiment_quality(result.outputs, example["outputs"])
                baseline_scores.append(score)

        baseline_avg = sum(baseline_scores) / len(baseline_scores) if baseline_scores else 0.0

        # Step 2: Format optimization
        # Use minimal format testing for speed
        from logillm.optimizers.format_optimizer import FormatOptimizerConfig, PromptFormat

        config = FormatOptimizerConfig(
            formats_to_test=[PromptFormat.MARKDOWN],  # Only test one format
            min_samples_per_format=1,
            max_samples_per_format=1,
        )
        format_optimizer = FormatOptimizer(metric=sentiment_quality, config=config)
        format_result = await format_optimizer.optimize(
            module=sentiment_module, dataset=sentiment_dataset
        )

        # Step 3: Hyperparameter optimization on format-optimized module
        hyper_optimizer = HyperparameterOptimizer(
            metric=sentiment_quality, strategy="random", n_trials=3
        )

        final_result = await hyper_optimizer.optimize(
            module=format_result.optimized_module,
            trainset=sentiment_dataset[:2],
            valset=sentiment_dataset[2:],
        )

        # Verify complete pipeline
        assert format_result.optimized_module is not None
        assert final_result.optimized_module is not None

        # Test final optimized performance
        final_module = final_result.optimized_module
        final_scores = []
        for example in sentiment_dataset:
            result = await final_module.forward(**example["inputs"])
            if result.success:
                score = sentiment_quality(result.outputs, example["outputs"])
                final_scores.append(score)

        final_avg = sum(final_scores) / len(final_scores) if final_scores else 0.0

        # Final should be at least as good as baseline
        assert final_avg >= baseline_avg - 0.2  # More tolerance for integration test

        # Should have optimization metadata from both steps
        assert "best_format" in format_result.metadata
        assert "best_config" in final_result.metadata

        print("Optimization Results:")
        print(f"Baseline: {baseline_avg:.3f}")
        print(f"After format optimization: {format_result.best_score:.3f}")
        print(f"After hyperparameter optimization: {final_avg:.3f}")
        print(f"Best format: {format_result.metadata['best_format']}")
        print(f"Best config: {final_result.metadata['best_config']}")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-m", "integration"])
