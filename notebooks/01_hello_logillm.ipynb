{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; margin-bottom: 20px;\">\n",
    "    <h1 style=\"color: white; margin: 0; font-size: 36px;\">üìö Notebook 1: Hello LogiLLM</h1>\n",
    "    <p style=\"color: rgba(255,255,255,0.9); margin-top: 10px; font-size: 18px;\">Your First Steps into Programming Language Models</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; margin-bottom: 20px;\">\n",
    "    <a href=\"README.md\" style=\"text-decoration: none; padding: 10px 20px; background: #f0f0f0; border-radius: 5px;\">‚Üê Back to Index</a>\n",
    "    <span style=\"padding: 10px 20px; background: #e8f5e9; border-radius: 5px;\">üü¢ Beginner ‚Ä¢ 10 minutes</span>\n",
    "    <a href=\"02_signatures.ipynb\" style=\"text-decoration: none; padding: 10px 20px; background: #f0f0f0; border-radius: 5px;\">Next: Signatures ‚Üí</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ What You'll Learn\n",
    "\n",
    "<div style=\"background: #f5f5f5; padding: 20px; border-radius: 10px; border-left: 4px solid #667eea;\">\n",
    "    <ul style=\"margin: 0; padding-left: 20px;\">\n",
    "        <li>‚úÖ <strong>Install and set up LogiLLM</strong> in under 30 seconds</li>\n",
    "        <li>‚úÖ <strong>Write your first LogiLLM program</strong> in 3 lines of code</li>\n",
    "        <li>‚úÖ <strong>Understand the core philosophy</strong>: Programming, not prompting</li>\n",
    "        <li>‚úÖ <strong>See real results</strong> with working examples</li>\n",
    "        <li>‚úÖ <strong>Debug and inspect</strong> what's happening under the hood</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "## üìã Prerequisites Check\n",
    "\n",
    "Let's make sure you're ready to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Python 3.13.4\n",
      "‚úÖ Python version is compatible!\n",
      "\n",
      "üîë API Keys:\n",
      "  OpenAI: ‚úÖ Found\n",
      "  Anthropic: ‚úÖ Found\n"
     ]
    }
   ],
   "source": [
    "# Check Python version (need 3.9+)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "python_version = sys.version_info\n",
    "print(f\"‚úÖ Python {python_version.major}.{python_version.minor}.{python_version.micro}\")\n",
    "\n",
    "if python_version.major < 3 or (python_version.major == 3 and python_version.minor < 9):\n",
    "    print(\"‚ö†Ô∏è Warning: LogiLLM requires Python 3.9 or higher\")\n",
    "else:\n",
    "    print(\"‚úÖ Python version is compatible!\")\n",
    "\n",
    "# Check for API keys\n",
    "has_openai = bool(os.getenv(\"OPENAI_API_KEY\"))\n",
    "has_anthropic = bool(os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "print(\"\\nüîë API Keys:\")\n",
    "print(f\"  OpenAI: {'‚úÖ Found' if has_openai else '‚ùå Not found (set OPENAI_API_KEY)'}\")\n",
    "print(f\"  Anthropic: {'‚úÖ Found' if has_anthropic else '‚ùå Not found (set ANTHROPIC_API_KEY)'}\")\n",
    "\n",
    "if not (has_openai or has_anthropic):\n",
    "    print(\"\\n‚ö†Ô∏è You'll need at least one API key to run the examples.\")\n",
    "    print(\"Set one with: export OPENAI_API_KEY=your_key_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Installation\n",
    "\n",
    "<div style=\"background: #fff3e0; padding: 15px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <strong>üéâ Fun Fact:</strong> LogiLLM's core has <strong>ZERO dependencies</strong>! Unlike DSPy which needs 15+ packages, LogiLLM uses only Python's standard library for its core functionality.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LogiLLM 0.2.6 installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install LogiLLM with OpenAI support\n",
    "#!pip install -q logillm[openai]\n",
    "!uv pip install -q logillm[openai]\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    import logillm\n",
    "    from logillm import __version__\n",
    "    print(f\"‚úÖ LogiLLM {__version__} installed successfully!\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Installation failed. Try: pip install logillm[openai]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåü Your First LogiLLM Program\n",
    "\n",
    "Let's write the classic \"Hello World\" of LLM programming - a simple question-answering system. Watch how LogiLLM makes this incredibly simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Answer: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from logillm.core.predict import Predict\n",
    "from logillm.providers import create_provider, register_provider\n",
    "\n",
    "# Setup (one-time configuration)\n",
    "provider = create_provider(\"openai\", model=\"gpt-4.1-mini\")  \n",
    "register_provider(provider, set_default=True)\n",
    "\n",
    "# Define what you want (not how to get it!)\n",
    "qa = Predict(\"question -> answer\")\n",
    "\n",
    "# Use it like a function\n",
    "result = await qa(question=\"What is the capital of France?\")\n",
    "\n",
    "print(f\"ü§ñ Answer: {result.outputs['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #e8f5e9; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h3 style=\"margin-top: 0;\">üéØ What Just Happened?</h3>\n",
    "    <ol>\n",
    "        <li><strong>No prompt engineering!</strong> You didn't write \"Please answer the following question...\"</li>\n",
    "        <li><strong>Structured output!</strong> The result is a proper object with fields, not raw text</li>\n",
    "        <li><strong>Type safety!</strong> LogiLLM knows 'question' is input and 'answer' is output</li>\n",
    "        <li><strong>Automatic parsing!</strong> The LLM's response was parsed into the structure you defined</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Understanding the Magic\n",
    "\n",
    "Let's peek under the hood to see what LogiLLM is actually doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debug Information:\n",
      "==================================================\n",
      "Answer: The sky appears blue because molecules in the Earth's atmosphere scatter sunlight in all directions\n",
      "\n",
      "üìù Actual prompt sent to LLM:\n",
      "--------------------------------------------------\n",
      "[USER]: [system]: Transform the inputs to outputs according to the field specifications below.\n",
      "\n",
      "Input fields:\n",
      "- question: question\n",
      "\n",
      "Output fields (provide these in your response):\n",
      "- answer: answer\n",
      "\n",
      "\n",
      "[user]: I...\n",
      "\n",
      "üìä Metadata:\n",
      "  Model: gpt-4.1-mini\n",
      "  Temperature: default\n"
     ]
    }
   ],
   "source": [
    "# Enable debug mode to see the actual prompts\n",
    "qa_debug = Predict(\"question -> answer\", debug=True)\n",
    "\n",
    "result = await qa_debug(question=\"Why is the sky blue?\")\n",
    "\n",
    "print(\"üîç Debug Information:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Answer: {result.outputs['answer']}\\n\")\n",
    "\n",
    "if result.prompt:\n",
    "    print(\"üìù Actual prompt sent to LLM:\")\n",
    "    print(\"-\" * 50)\n",
    "    messages = result.prompt.get('messages', [])\n",
    "    for msg in messages:\n",
    "        role = msg.get('role', 'unknown')\n",
    "        content = msg.get('content', '')[:200]  # First 200 chars\n",
    "        print(f\"[{role.upper()}]: {content}...\")\n",
    "    \n",
    "    print(\"\\nüìä Metadata:\")\n",
    "    print(f\"  Model: {result.prompt.get('model', 'unknown')}\")\n",
    "    print(f\"  Temperature: {result.prompt.get('temperature', 'default')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Different Ways to Define Programs\n",
    "\n",
    "LogiLLM gives you multiple ways to define what you want. Choose the style that fits your needs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style 1: String Signatures (Quick & Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: Hola\n",
      "\n",
      "Summary: LogiLLM is a framework designed for programming language models.\n",
      "\n",
      "Sentiment: positive\n",
      "Confidence: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Simple transformation\n",
    "translator = Predict(\"text -> translation\")\n",
    "result = await translator(text=\"Hello, world!\")\n",
    "print(f\"Translation: {result.outputs['translation']}\")\n",
    "\n",
    "# Multiple inputs\n",
    "summarizer = Predict(\"title, content -> summary\")\n",
    "result = await summarizer(\n",
    "    title=\"LogiLLM Introduction\",\n",
    "    content=\"LogiLLM is a framework for programming language models...\"\n",
    ")\n",
    "print(f\"\\nSummary: {result.outputs['summary']}\")\n",
    "\n",
    "# Multiple outputs with types\n",
    "analyzer = Predict(\"text -> sentiment: str, confidence: float\")\n",
    "result = await analyzer(text=\"I love this framework!\")\n",
    "print(f\"\\nSentiment: {result.outputs['sentiment']}\")\n",
    "print(f\"Confidence: {result.outputs['confidence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style 2: Class-Based Signatures (Structured & Documented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Review Analysis:\n",
      "  Sentiment: Positive\n",
      "  Rating: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
      "  Would Recommend: ‚úÖ\n",
      "  Key Points:\n",
      "    ‚Ä¢ Battery lasts all day\n",
      "    ‚Ä¢ Beautiful screen\n",
      "    ‚Ä¢ Super fast performance\n",
      "    ‚Ä¢ Slightly heavy\n"
     ]
    }
   ],
   "source": [
    "from logillm.core.signatures import Signature, InputField, OutputField\n",
    "\n",
    "class ProductReview(Signature):\n",
    "    \"\"\"Analyze customer product reviews.\"\"\"\n",
    "    \n",
    "    # Input fields with descriptions\n",
    "    review_text: str = InputField(desc=\"The customer's review text\")\n",
    "    product_name: str = InputField(desc=\"Name of the product being reviewed\")\n",
    "    \n",
    "    # Output fields with types and descriptions\n",
    "    sentiment: str = OutputField(desc=\"positive, negative, or neutral\")\n",
    "    rating: int = OutputField(desc=\"Predicted rating from 1-5 stars\")\n",
    "    key_points: list[str] = OutputField(desc=\"Main points mentioned in the review\")\n",
    "    recommend: bool = OutputField(desc=\"Would the reviewer recommend this product?\")\n",
    "\n",
    "# Use the structured signature\n",
    "review_analyzer = Predict(ProductReview)\n",
    "\n",
    "result = await review_analyzer(\n",
    "    review_text=\"\"\"This laptop is amazing! The battery lasts all day, \n",
    "                   the screen is beautiful, and it's super fast. \n",
    "                   Only downside is it's a bit heavy.\"\"\",\n",
    "    product_name=\"TechBook Pro 2024\"\n",
    ")\n",
    "\n",
    "print(\"üìä Review Analysis:\")\n",
    "print(f\"  Sentiment: {result.outputs['sentiment']}\")\n",
    "print(f\"  Rating: {'‚≠ê' * result.outputs['rating']}\")\n",
    "print(f\"  Would Recommend: {'‚úÖ' if result.outputs['recommend'] else '‚ùå'}\")\n",
    "print(f\"  Key Points:\")\n",
    "for point in result.outputs['key_points']:\n",
    "    print(f\"    ‚Ä¢ {point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Experiment: Compare Traditional Prompting vs LogiLLM\n",
    "\n",
    "Let's see the difference between traditional prompt engineering and LogiLLM's programming approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ LogiLLM Result (automatic parsing):\n",
      "  Intent: Request for report submission\n",
      "  Urgency: High (due by tomorrow morning)\n",
      "  Tone: Urgent and serious\n",
      "  Action Items: ['Prepare', 'submit the report by tomorrow morning']\n",
      "\n",
      "‚ú® Benefits of LogiLLM approach:\n",
      "  ‚Ä¢ No prompt engineering needed\n",
      "  ‚Ä¢ Automatic output parsing\n",
      "  ‚Ä¢ Type safety and validation\n",
      "  ‚Ä¢ Reusable and composable\n",
      "  ‚Ä¢ Can be optimized automatically\n"
     ]
    }
   ],
   "source": [
    "# Traditional approach (what you'd normally do)\n",
    "traditional_prompt = \"\"\"\n",
    "Please analyze the following email and extract:\n",
    "1. The sender's intent (request, complaint, inquiry, or other)\n",
    "2. The urgency level (high, medium, or low)\n",
    "3. Any action items mentioned\n",
    "4. The overall tone (positive, negative, or neutral)\n",
    "\n",
    "Email: \"I need the report by tomorrow morning! This is critical for the board meeting.\"\n",
    "\n",
    "Please format your response as JSON.\n",
    "\"\"\"\n",
    "\n",
    "# LogiLLM approach (programming, not prompting)\n",
    "class EmailAnalysis(Signature):\n",
    "    \"\"\"Analyze email content for key information.\"\"\"\n",
    "    email: str = InputField()\n",
    "    intent: str = OutputField(desc=\"request, complaint, inquiry, or other\")\n",
    "    urgency: str = OutputField(desc=\"high, medium, or low\")\n",
    "    action_items: list[str] = OutputField(desc=\"List of action items\")\n",
    "    tone: str = OutputField(desc=\"positive, negative, or neutral\")\n",
    "\n",
    "email_analyzer = Predict(EmailAnalysis)\n",
    "result = await email_analyzer(\n",
    "    email=\"I need the report by tomorrow morning! This is critical for the board meeting.\"\n",
    ")\n",
    "\n",
    "print(\"üéØ LogiLLM Result (automatic parsing):\")\n",
    "print(f\"  Intent: {result.outputs['intent']}\")\n",
    "print(f\"  Urgency: {result.outputs['urgency']}\")\n",
    "print(f\"  Tone: {result.outputs['tone']}\")\n",
    "print(f\"  Action Items: {result.outputs['action_items']}\")\n",
    "\n",
    "print(\"\\n‚ú® Benefits of LogiLLM approach:\")\n",
    "print(\"  ‚Ä¢ No prompt engineering needed\")\n",
    "print(\"  ‚Ä¢ Automatic output parsing\")\n",
    "print(\"  ‚Ä¢ Type safety and validation\")\n",
    "print(\"  ‚Ä¢ Reusable and composable\")\n",
    "print(\"  ‚Ä¢ Can be optimized automatically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Core Philosophy: Programming vs Prompting\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 10px; color: white; margin: 20px 0;\">\n",
    "    <h3 style=\"margin-top: 0; color: white;\">The LogiLLM Way</h3>\n",
    "    <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px;\">\n",
    "        <div>\n",
    "            <h4 style=\"color: rgba(255,255,255,0.9);\">‚ùå Traditional (Prompting)</h4>\n",
    "            <ul style=\"margin: 10px 0;\">\n",
    "                <li>Write specific prompt strings</li>\n",
    "                <li>Manually parse responses</li>\n",
    "                <li>Hope the format is consistent</li>\n",
    "                <li>Rewrite for each use case</li>\n",
    "                <li>Trial and error optimization</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h4 style=\"color: rgba(255,255,255,0.9);\">‚úÖ LogiLLM (Programming)</h4>\n",
    "            <ul style=\"margin: 10px 0;\">\n",
    "                <li>Define input/output contracts</li>\n",
    "                <li>Automatic parsing & validation</li>\n",
    "                <li>Guaranteed structure</li>\n",
    "                <li>Reusable components</li>\n",
    "                <li>Automatic optimization</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Interactive Exercise: Build Your Own\n",
    "\n",
    "Now it's your turn! Let's build a simple recipe analyzer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üçï Recipe Analysis:\n",
      "  cuisine_type: Italian\n",
      "  difficulty: Easy\n",
      "  prep_time_minutes: 5\n",
      "  main_ingredients: ['pizza dough', 'tomato sauce', 'fresh mozzarella', 'basil']\n",
      "  dietary_tags: ['vegetarian']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Complete this signature for analyzing recipes\n",
    "class RecipeAnalyzer(Signature):\n",
    "    \"\"\"Analyze a recipe for key information.\"\"\"\n",
    "    \n",
    "    # Add input field for recipe text\n",
    "    recipe: str = InputField(desc=\"The recipe text to analyze\")\n",
    "    \n",
    "    # TODO: Add these output fields:\n",
    "    # - cuisine_type: str (Italian, Chinese, Mexican, etc.)\n",
    "    # - difficulty: str (easy, medium, hard)\n",
    "    # - prep_time_minutes: int\n",
    "    # - main_ingredients: list[str]\n",
    "    # - dietary_tags: list[str] (vegetarian, gluten-free, etc.)\n",
    "    \n",
    "    cuisine_type: str = OutputField(desc=\"Type of cuisine\")\n",
    "    difficulty: str = OutputField(desc=\"easy, medium, or hard\")\n",
    "    prep_time_minutes: int = OutputField(desc=\"Estimated preparation time\")\n",
    "    main_ingredients: list[str] = OutputField(desc=\"List of main ingredients\")\n",
    "    dietary_tags: list[str] = OutputField(desc=\"Dietary categories\")\n",
    "\n",
    "# Test your analyzer\n",
    "analyzer = Predict(RecipeAnalyzer)\n",
    "result = await analyzer(\n",
    "    recipe=\"\"\"Quick Margherita Pizza: \n",
    "    Spread tomato sauce on pizza dough, add fresh mozzarella and basil. \n",
    "    Bake at 450¬∞F for 12-15 minutes. Perfect for beginners!\"\"\"\n",
    ")\n",
    "\n",
    "print(\"üçï Recipe Analysis:\")\n",
    "for key, value in result.outputs.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Usage Tracking & Costs\n",
    "\n",
    "LogiLLM automatically tracks token usage and costs. Let's see how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is machine learning?...\n",
      "   Tokens: 93 (input: 55, output: 38)\n",
      "Q: Explain quantum computing in s...\n",
      "   Tokens: 135 (input: 57, output: 78)\n",
      "Q: What are the benefits of exerc...\n",
      "   Tokens: 108 (input: 57, output: 51)\n",
      "\n",
      "üìä Total tokens used: 336\n",
      "üí∞ Estimated cost: $0.0007 (at $0.002/1K tokens)\n"
     ]
    }
   ],
   "source": [
    "# Run a few queries and track usage\n",
    "qa = Predict(\"question -> answer\")\n",
    "\n",
    "questions = [\n",
    "  \"What is machine learning?\",\n",
    "  \"Explain quantum computing in simple terms\",\n",
    "  \"What are the benefits of exercise?\"\n",
    "]\n",
    "\n",
    "total_tokens = 0\n",
    "for q in questions:\n",
    "  result = await qa(question=q)\n",
    "  if result.usage and result.usage.tokens:\n",
    "      tokens = result.usage.tokens.total_tokens\n",
    "      total_tokens += tokens\n",
    "      print(f\"Q: {q[:30]}...\")\n",
    "      print(f\"   Tokens: {tokens} (input: {result.usage.tokens.input_tokens}, output: {result.usage.tokens.output_tokens})\")\n",
    "\n",
    "print(f\"\\nüìä Total tokens used: {total_tokens}\")\n",
    "print(f\"üí∞ Estimated cost: ${total_tokens * 0.000002:.4f} (at $0.002/1K tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Performance Tips\n",
    "\n",
    "<div style=\"background: #fff8e1; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h3 style=\"margin-top: 0;\">‚ö° Speed & Efficiency</h3>\n",
    "    <ul>\n",
    "        <li><strong>Batch requests</strong> when possible to reduce latency</li>\n",
    "        <li><strong>Use appropriate models</strong> - don't use GPT-4 for simple tasks</li>\n",
    "        <li><strong>Cache results</strong> for repeated queries</li>\n",
    "        <li><strong>Optimize signatures</strong> - we'll cover this in Notebook 5</li>\n",
    "        <li><strong>Use async</strong> for concurrent processing</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è Sequential: 3.11s\n",
      "‚ö° Concurrent: 1.55s\n",
      "üöÄ Speedup: 2.0x faster!\n"
     ]
    }
   ],
   "source": [
    "# Example: Concurrent processing with async\n",
    "import time\n",
    "\n",
    "qa = Predict(\"question -> answer\")\n",
    "questions = [\n",
    "    \"What is Python?\",\n",
    "    \"What is JavaScript?\", \n",
    "    \"What is Rust?\"\n",
    "]\n",
    "\n",
    "# Sequential (slow)\n",
    "start = time.time()\n",
    "for q in questions:\n",
    "    await qa(question=q)\n",
    "sequential_time = time.time() - start\n",
    "\n",
    "# Concurrent (fast!)\n",
    "start = time.time()\n",
    "tasks = [qa(question=q) for q in questions]\n",
    "results = await asyncio.gather(*tasks)\n",
    "concurrent_time = time.time() - start\n",
    "\n",
    "print(f\"‚è±Ô∏è Sequential: {sequential_time:.2f}s\")\n",
    "print(f\"‚ö° Concurrent: {concurrent_time:.2f}s\")\n",
    "print(f\"üöÄ Speedup: {sequential_time/concurrent_time:.1f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary & Key Takeaways\n",
    "\n",
    "<div style=\"background: #e8f5e9; padding: 25px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h3 style=\"margin-top: 0;\">What You've Learned</h3>\n",
    "    <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px;\">\n",
    "        <div>\n",
    "            <h4>‚úÖ Concepts</h4>\n",
    "            <ul>\n",
    "                <li>Programming vs Prompting paradigm</li>\n",
    "                <li>Signatures define contracts</li>\n",
    "                <li>Automatic parsing & validation</li>\n",
    "                <li>Zero-dependency philosophy</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h4>‚úÖ Skills</h4>\n",
    "            <ul>\n",
    "                <li>Install and configure LogiLLM</li>\n",
    "                <li>Create string signatures</li>\n",
    "                <li>Build class-based signatures</li>\n",
    "                <li>Debug and inspect prompts</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Progress Check\n",
    "\n",
    "Run this cell to track your progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Notebook 1 Progress: 7/7 sections (100%)\n",
      "\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "‚úÖ Completed sections:\n",
      "  ‚Ä¢ Installation\n",
      "  ‚Ä¢ First Program\n",
      "  ‚Ä¢ Debug Mode\n",
      "  ‚Ä¢ String Signatures\n",
      "  ‚Ä¢ Class Signatures\n",
      "  ‚Ä¢ Exercise\n",
      "  ‚Ä¢ Performance\n",
      "\n",
      "üéâ Congratulations! You've completed Notebook 1!\n",
      "Ready to move on to Notebook 2: Signatures\n"
     ]
    }
   ],
   "source": [
    "# Progress tracker\n",
    "completed = {\n",
    "    \"installation\": True,\n",
    "    \"first_program\": True,\n",
    "    \"debug_mode\": True,\n",
    "    \"string_signatures\": True,\n",
    "    \"class_signatures\": True,\n",
    "    \"exercise\": True,\n",
    "    \"performance\": True\n",
    "}\n",
    "\n",
    "total = len(completed)\n",
    "done = sum(completed.values())\n",
    "percentage = (done / total) * 100\n",
    "\n",
    "print(f\"üìä Notebook 1 Progress: {done}/{total} sections ({percentage:.0f}%)\")\n",
    "print(\"\\n\" + \"‚ñà\" * int(percentage // 5) + \"‚ñë\" * (20 - int(percentage // 5)))\n",
    "print(\"\\n‚úÖ Completed sections:\")\n",
    "for section, status in completed.items():\n",
    "    if status:\n",
    "        print(f\"  ‚Ä¢ {section.replace('_', ' ').title()}\")\n",
    "\n",
    "if percentage == 100:\n",
    "    print(\"\\nüéâ Congratulations! You've completed Notebook 1!\")\n",
    "    print(\"Ready to move on to Notebook 2: Signatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Additional Resources\n",
    "\n",
    "- üìñ [LogiLLM Documentation](../docs/README.md)\n",
    "- üíª [Example Code](../examples/)\n",
    "- üîß [API Reference](../docs/api-reference/)\n",
    "- üí¨ [Community Discord](https://discord.gg/logillm)\n",
    "\n",
    "## ü§î Reflection Questions\n",
    "\n",
    "Before moving on, consider:\n",
    "1. How is LogiLLM's approach different from writing prompts?\n",
    "2. What are the benefits of defining signatures?\n",
    "3. When would you use string vs class-based signatures?\n",
    "4. How could you use LogiLLM in your own projects?\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; margin-top: 40px; padding: 20px; background: #f5f5f5; border-radius: 10px;\">\n",
    "    <a href=\"README.md\" style=\"text-decoration: none; padding: 10px 20px; background: white; border-radius: 5px; border: 1px solid #ddd;\">‚Üê Back to Index</a>\n",
    "    <div style=\"text-align: center;\">\n",
    "        <strong>Great job completing Notebook 1! üéâ</strong>\n",
    "    </div>\n",
    "    <a href=\"02_signatures.ipynb\" style=\"text-decoration: none; padding: 10px 20px; background: #667eea; color: white; border-radius: 5px;\">Continue to Notebook 2 ‚Üí</a>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
