{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; margin-bottom: 20px;\">\n",
    "    <h1 style=\"color: white; margin: 0; font-size: 36px;\">ğŸ” Notebook 4: Debugging, Logging & Persistence</h1>\n",
    "    <p style=\"color: rgba(255,255,255,0.9); margin-top: 10px; font-size: 18px;\">Understanding, Monitoring, and Saving Your LLM Applications</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; margin-bottom: 20px;\">\n",
    "    <a href=\"03_modules_adapters.ipynb\" style=\"text-decoration: none; padding: 10px 20px; background: #f0f0f0; border-radius: 5px;\">â† Notebook 3</a>\n",
    "    <span style=\"padding: 10px 20px; background: #fff8e1; border-radius: 5px;\">ğŸŸ¡ Intermediate â€¢ 25 minutes</span>\n",
    "    <a href=\"05_optimization.ipynb\" style=\"text-decoration: none; padding: 10px 20px; background: #f0f0f0; border-radius: 5px;\">Notebook 5 â†’</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ What You'll Learn\n",
    "\n",
    "<div style=\"background: #f5f5f5; padding: 20px; border-radius: 10px; border-left: 4px solid #667eea;\">\n",
    "    <h3>ğŸ” Debugging - See Inside Your LLM</h3>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px;\">\n",
    "        <li>âœ… <strong>Debug Mode</strong>: Inspect every request and response</li>\n",
    "        <li>âœ… <strong>Token Tracking</strong>: Monitor usage and costs</li>\n",
    "        <li>âœ… <strong>Prompt Inspection</strong>: See exactly what's sent to the LLM</li>\n",
    "        <li>âœ… <strong>Error Analysis</strong>: Understand and fix failures</li>\n",
    "    </ul>    \n",
    "    <h3>ğŸ“Š Logging & Callbacks - Production Monitoring</h3>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px;\">\n",
    "        <li>âœ… <strong>JSONL Logging</strong>: Structured logs for analysis</li>\n",
    "        <li>âœ… <strong>Custom Callbacks</strong>: Monitor your specific needs</li>\n",
    "        <li>âœ… <strong>Performance Tracking</strong>: Latency and throughput metrics</li>\n",
    "        <li>âœ… <strong>Cost Management</strong>: Track API spending</li>\n",
    "    </ul>    \n",
    "    <h3>ğŸ’¾ Persistence - Save & Load Your Work</h3>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px;\">\n",
    "        <li>âœ… <strong>Save Modules</strong>: Persist configured modules</li>\n",
    "        <li>âœ… <strong>Load & Resume</strong>: Continue from saved state</li>\n",
    "        <li>âœ… <strong>Optimization Results</strong>: Store and reuse optimized modules</li>\n",
    "        <li>âœ… <strong>Production Deployment</strong>: Package for deployment</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "## ğŸ”§ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI API key found\n",
      "âœ… LogiLLM 0.2.16 ready with gpt-4.1-mini!\n",
      "ğŸ“ Working directory: /tmp/logillm_notebook_c36fth53\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import logillm\n",
    "from logillm.core.predict import Predict, ChainOfThought\n",
    "from logillm.core.signatures import Signature, InputField, OutputField\n",
    "from logillm.core.callbacks import CallbackManager, AbstractCallback, ModuleEndEvent\n",
    "from logillm.core.jsonl_callback import JSONLCallback, register_jsonl_logger\n",
    "from logillm.providers import create_provider, register_provider\n",
    "\n",
    "# Check API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"âš ï¸ WARNING: OPENAI_API_KEY not set!\")\n",
    "    print(\"Set it with: export OPENAI_API_KEY=your_key\")\n",
    "else:\n",
    "    print(\"âœ… OpenAI API key found\")\n",
    "\n",
    "# Setup provider\n",
    "try:\n",
    "    provider = create_provider(\"openai\", model=\"gpt-4.1-mini\")\n",
    "    register_provider(provider, set_default=True)\n",
    "    print(f\"âœ… LogiLLM {logillm.__version__} ready with {provider.model}!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "\n",
    "# Create temp directory for examples\n",
    "TEMP_DIR = Path(tempfile.mkdtemp(prefix=\"logillm_notebook_\"))\n",
    "print(f\"ğŸ“ Working directory: {TEMP_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Part 1: Debugging - See Inside Your LLM\n",
    "\n",
    "Understanding what's happening inside your LLM calls is crucial for development and troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Debug Mode Example:\n",
      "============================================================\n",
      "\n",
      "âœ… Answer: The capital of France is Paris.\n",
      "\n",
      "ğŸ“Š Debug Information Available:\n",
      "  â€¢ Has request data: True\n",
      "  â€¢ Has response data: True\n",
      "  â€¢ Has prompt data: True\n"
     ]
    }
   ],
   "source": [
    "# Define a simple signature for testing\n",
    "class SimpleQA(Signature):\n",
    "    \"\"\"Answer questions concisely.\"\"\"\n",
    "    question: str = InputField(desc=\"The question to answer\")\n",
    "    answer: str = OutputField(desc=\"A concise answer\")\n",
    "\n",
    "# Create module WITH debug mode enabled\n",
    "qa_debug = Predict(SimpleQA, debug=True)\n",
    "\n",
    "print(\"ğŸ” Debug Mode Example:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Make a call - debug info will be captured\n",
    "result = await qa_debug(question=\"What is the capital of France?\")\n",
    "\n",
    "# Now we have access to complete request/response data!\n",
    "print(f\"\\nâœ… Answer: {result.outputs['answer']}\")\n",
    "print(f\"\\nğŸ“Š Debug Information Available:\")\n",
    "print(f\"  â€¢ Has request data: {hasattr(result, 'request')}\")\n",
    "print(f\"  â€¢ Has response data: {hasattr(result, 'response')}\")\n",
    "print(f\"  â€¢ Has prompt data: {hasattr(result, 'prompt')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¤ REQUEST DATA:\n",
      "============================================================\n",
      "Provider: openai\n",
      "Model: gpt-4.1-mini\n",
      "Adapter: chat\n",
      "\n",
      "Messages sent (2 total):\n",
      "  [0] Role: system\n",
      "      Content: Answer questions concisely....\n",
      "  [1] Role: user\n",
      "      Content: [system]: Task: Answer questions concisely.\n",
      "\n",
      "Input fields:\n",
      "- question: The question\n",
      "\n",
      "Output fields (...\n",
      "\n",
      "ğŸ“¥ RESPONSE DATA:\n",
      "============================================================\n",
      "Text length: 39 chars\n",
      "Finish reason: stop\n",
      "Latency: N/A\n",
      "\n",
      "ğŸ’° Token Usage:\n",
      "  Input tokens: 84\n",
      "  Output tokens: 9\n",
      "  Total tokens: 93\n",
      "  Estimated cost: N/A\n"
     ]
    }
   ],
   "source": [
    "# Inspect the request data (what was sent to the LLM)\n",
    "print(\"ğŸ“¤ REQUEST DATA:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if hasattr(result, 'request') and result.request:\n",
    "    print(f\"Provider: {result.request.get('provider', 'N/A')}\")\n",
    "    print(f\"Model: {result.request.get('model', 'N/A')}\")\n",
    "    print(f\"Adapter: {result.request.get('adapter', 'N/A')}\")\n",
    "    \n",
    "    # Show the actual messages sent\n",
    "    messages = result.request.get('messages', [])\n",
    "    print(f\"\\nMessages sent ({len(messages)} total):\")\n",
    "    for i, msg in enumerate(messages[:2]):  # Show first 2 messages\n",
    "        print(f\"  [{i}] Role: {msg.get('role', 'unknown')}\")\n",
    "        content = msg.get('content', '')[:100]  # First 100 chars\n",
    "        print(f\"      Content: {content}...\")\n",
    "\n",
    "print(\"\\nğŸ“¥ RESPONSE DATA:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if hasattr(result, 'response') and result.response:\n",
    "    text_content = result.response.get('text', '')\n",
    "    print(f\"Text length: {len(text_content)} chars\")\n",
    "    print(f\"Finish reason: {result.response.get('finish_reason', 'N/A')}\")\n",
    "    \n",
    "    # Handle latency that might be None\n",
    "    latency = result.response.get('latency')\n",
    "    if latency is not None:\n",
    "        print(f\"Latency: {latency:.2f} seconds\")\n",
    "    else:\n",
    "        print(\"Latency: N/A\")\n",
    "    \n",
    "    # Token usage and costs\n",
    "    usage = result.response.get('usage', {})\n",
    "    print(f\"\\nğŸ’° Token Usage:\")\n",
    "    print(f\"  Input tokens: {usage.get('input_tokens', 0)}\")\n",
    "    print(f\"  Output tokens: {usage.get('output_tokens', 0)}\")\n",
    "    print(f\"  Total tokens: {usage.get('total_tokens', 0)}\")\n",
    "    \n",
    "    cost = result.response.get('cost')\n",
    "    if cost is not None and cost > 0:\n",
    "        print(f\"  Estimated cost: ${cost:.5f}\")\n",
    "    else:\n",
    "        print(\"  Estimated cost: N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Three Ways to Enable Debug Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: Debug enabled at creation\n",
      "Method 2: Debug enabled dynamically\n",
      "Method 3: Set LOGILLM_DEBUG=1 environment variable\n",
      "\n",
      "âœ… Debug can be toggled on/off as needed!\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Enable at module creation\n",
    "qa1 = Predict(SimpleQA, debug=True)\n",
    "print(\"Method 1: Debug enabled at creation\")\n",
    "\n",
    "# Method 2: Toggle debug dynamically\n",
    "qa2 = Predict(SimpleQA)  # Created without debug\n",
    "qa2.enable_debug_mode()  # Turn it on\n",
    "print(\"Method 2: Debug enabled dynamically\")\n",
    "\n",
    "# Method 3: Environment variable (affects all modules)\n",
    "# os.environ['LOGILLM_DEBUG'] = '1'\n",
    "# qa3 = Predict(SimpleQA)  # Will have debug enabled\n",
    "print(\"Method 3: Set LOGILLM_DEBUG=1 environment variable\")\n",
    "\n",
    "# You can also disable debug mode\n",
    "qa1.disable_debug_mode()\n",
    "print(\"\\nâœ… Debug can be toggled on/off as needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ› Practical Example: Debugging a Complex Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Debugging Complex Extraction:\n",
      "============================================================\n",
      "\n",
      "âœ… Extraction Results:\n",
      "  Entities: ['Apple', 'Tim Cook']\n",
      "  Numbers: [94.8, 4]\n",
      "  Dates: ['Q4 2023', 'November 2nd']\n",
      "\n",
      "ğŸ’­ LLM Reasoning:\n",
      "  Step 1: Identify named entities such as organizations and people. Here, 'Apple' and 'Tim Cook' are named entities. Step 2: Extract numbers mentioned, including financial figures like '$94.8 billion'. ...\n"
     ]
    }
   ],
   "source": [
    "# A more complex signature that might have issues\n",
    "class DataExtraction(Signature):\n",
    "    \"\"\"Extract structured data from text.\"\"\"\n",
    "    text: str = InputField(desc=\"Text to analyze\")\n",
    "    \n",
    "    entities: list[str] = OutputField(desc=\"Named entities found\")\n",
    "    numbers: list[float] = OutputField(desc=\"Numbers mentioned\")\n",
    "    dates: list[str] = OutputField(desc=\"Dates mentioned\")\n",
    "\n",
    "# Enable debug to understand what's happening\n",
    "extractor = ChainOfThought(DataExtraction, debug=True)\n",
    "\n",
    "test_text = \"\"\"\n",
    "Apple reported $94.8 billion in revenue for Q4 2023, beating estimates.\n",
    "CEO Tim Cook announced the results on November 2nd, highlighting strong iPhone sales.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸ” Debugging Complex Extraction:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = await extractor(text=test_text)\n",
    "\n",
    "if result.success:\n",
    "    print(\"\\nâœ… Extraction Results:\")\n",
    "    print(f\"  Entities: {result.outputs['entities']}\")\n",
    "    print(f\"  Numbers: {result.outputs['numbers']}\")\n",
    "    print(f\"  Dates: {result.outputs['dates']}\")\n",
    "    \n",
    "    # Debug helps us understand the reasoning\n",
    "    if 'reasoning' in result.outputs:\n",
    "        print(f\"\\nğŸ’­ LLM Reasoning:\")\n",
    "        print(f\"  {result.outputs['reasoning'][:200]}...\")\n",
    "else:\n",
    "    print(\"\\nâŒ Extraction failed!\")\n",
    "    # Debug mode lets us see why\n",
    "    if hasattr(result, 'response'):\n",
    "        print(f\"Error details: {result.response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Part 2: Logging & Callbacks - Production Monitoring\n",
    "\n",
    "For production systems, you need structured logging to track performance, costs, and debug issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Logging to: /tmp/logillm_notebook_c36fth53/execution.jsonl\n",
      "ğŸ“Œ Callback ID: JSONLCallback_138711150710464\n",
      "\n",
      "ğŸ”„ Making API calls...\n",
      "  Q: What is the speed of light?... â†’ A: The speed of light in a vacuum is approximately 29...\n",
      "  Q: Who wrote Romeo and Juliet?... â†’ A: William Shakespeare wrote Romeo and Juliet....\n",
      "  Q: What is machine learning?... â†’ A: Machine learning is a field of artificial intellig...\n",
      "\n",
      "âœ… All calls logged to execution.jsonl\n"
     ]
    }
   ],
   "source": [
    "# JSONL logging captures everything in structured format\n",
    "log_file = TEMP_DIR / \"execution.jsonl\"\n",
    "\n",
    "# Register the JSONL logger\n",
    "callback_id = register_jsonl_logger(\n",
    "    str(log_file),\n",
    "    include_module_events=True,    # Log module start/end\n",
    "    include_provider_events=True,  # Log LLM requests/responses\n",
    "    include_optimization_events=False  # Not optimizing yet\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“ Logging to: {log_file}\")\n",
    "print(f\"ğŸ“Œ Callback ID: {callback_id}\")\n",
    "\n",
    "# Now all module executions will be logged!\n",
    "qa = Predict(SimpleQA)\n",
    "\n",
    "# Make several calls - all will be logged\n",
    "questions = [\n",
    "    \"What is the speed of light?\",\n",
    "    \"Who wrote Romeo and Juliet?\",\n",
    "    \"What is machine learning?\"\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ”„ Making API calls...\")\n",
    "for q in questions:\n",
    "    result = await qa(question=q)\n",
    "    print(f\"  Q: {q[:30]}... â†’ A: {result.outputs['answer'][:50]}...\")\n",
    "\n",
    "print(f\"\\nâœ… All calls logged to {log_file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Analyzing Logged Data:\n",
      "============================================================\n",
      "\n",
      "Total events logged: 12\n",
      "\n",
      "ğŸ“ˆ Event Type Distribution:\n",
      "  module_start: 3\n",
      "  provider_request: 3\n",
      "  provider_response: 3\n",
      "  module_end: 3\n",
      "\n",
      "ğŸ’° Usage Summary:\n",
      "  Total tokens: 321\n",
      "  Total cost: $0.00000\n",
      "  Total latency: 2.47s\n",
      "  Average latency: 0.82s per call\n",
      "\n",
      "ğŸ“ Sample Event Structure:\n",
      "  Type: module_start\n",
      "  Timestamp: 2025-09-03T08:01:17.355676\n",
      "  Module: Predict\n",
      "  Call ID: c289f926-a901-4e12-9...\n"
     ]
    }
   ],
   "source": [
    "# Analyze the logged data\n",
    "print(\"ğŸ“Š Analyzing Logged Data:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "events = []\n",
    "with open(log_file, 'r') as f:\n",
    "    for line in f:\n",
    "        events.append(json.loads(line))\n",
    "\n",
    "print(f\"\\nTotal events logged: {len(events)}\")\n",
    "\n",
    "# Analyze event types\n",
    "event_types = {}\n",
    "for event in events:\n",
    "    event_type = event.get('event_type', 'unknown')\n",
    "    event_types[event_type] = event_types.get(event_type, 0) + 1\n",
    "\n",
    "print(\"\\nğŸ“ˆ Event Type Distribution:\")\n",
    "for event_type, count in event_types.items():\n",
    "    print(f\"  {event_type}: {count}\")\n",
    "\n",
    "# Calculate total cost and tokens\n",
    "total_cost = 0\n",
    "total_tokens = 0\n",
    "total_latency = 0\n",
    "provider_response_count = 0\n",
    "\n",
    "for event in events:\n",
    "    if event.get('event_type') == 'provider_response':\n",
    "        provider_response_count += 1\n",
    "        usage = event.get('usage', {})\n",
    "        total_tokens += usage.get('total_tokens', 0)\n",
    "        \n",
    "        cost = event.get('cost')\n",
    "        if cost is not None:\n",
    "            total_cost += cost\n",
    "        \n",
    "        duration = event.get('duration')\n",
    "        if duration is not None:\n",
    "            total_latency += duration\n",
    "\n",
    "print(\"\\nğŸ’° Usage Summary:\")\n",
    "print(f\"  Total tokens: {total_tokens:,}\")\n",
    "\n",
    "if total_cost > 0:\n",
    "    print(f\"  Total cost: ${total_cost:.5f}\")\n",
    "else:\n",
    "    print(\"  Total cost: $0.00000\")\n",
    "\n",
    "if total_latency > 0:\n",
    "    print(f\"  Total latency: {total_latency:.2f}s\")\n",
    "    if provider_response_count > 0:\n",
    "        avg_latency = total_latency / provider_response_count\n",
    "        print(f\"  Average latency: {avg_latency:.2f}s per call\")\n",
    "else:\n",
    "    print(\"  Total latency: N/A\")\n",
    "\n",
    "# Show a sample event\n",
    "print(\"\\nğŸ“ Sample Event Structure:\")\n",
    "if events:\n",
    "    sample = events[0]\n",
    "    print(f\"  Type: {sample.get('event_type')}\")\n",
    "    print(f\"  Timestamp: {sample.get('timestamp')}\")\n",
    "    print(f\"  Module: {sample.get('module_name', 'N/A')}\")\n",
    "    \n",
    "    call_id = sample.get('call_id', 'N/A')\n",
    "    if call_id and call_id != 'N/A':\n",
    "        print(f\"  Call ID: {str(call_id)[:20]}...\")\n",
    "    else:\n",
    "        print(\"  Call ID: N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¨ Custom Callbacks for Specific Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Making tracked calls...\n",
      "ğŸ’° Predict call #1: No cost data\n",
      "ğŸ’° Predict call #2: No cost data\n",
      "\n",
      "ğŸ“Š Cost Tracking Summary:\n",
      "========================================\n",
      "Total calls: 2\n",
      "Total cost: No cost data available\n"
     ]
    }
   ],
   "source": [
    "# Create a custom callback for cost tracking\n",
    "class CostTracker(AbstractCallback):\n",
    "    def __init__(self):\n",
    "        self.total_cost = 0\n",
    "        self.call_count = 0\n",
    "        self.costs_by_module = {}\n",
    "    \n",
    "    async def on_module_end(self, event: ModuleEndEvent):\n",
    "        \"\"\"Track costs when modules complete.\"\"\"\n",
    "        self.call_count += 1\n",
    "        \n",
    "        # Get module name\n",
    "        module_name = event.module.__class__.__name__\n",
    "        \n",
    "        # Extract cost from prediction if available\n",
    "        cost = None\n",
    "        if hasattr(event, 'prediction') and event.prediction:\n",
    "            if hasattr(event.prediction, 'response') and event.prediction.response:\n",
    "                cost = event.prediction.response.get('cost')\n",
    "        \n",
    "        if cost is not None and cost > 0:\n",
    "            self.total_cost += cost\n",
    "            \n",
    "            # Track by module\n",
    "            if module_name not in self.costs_by_module:\n",
    "                self.costs_by_module[module_name] = 0\n",
    "            self.costs_by_module[module_name] += cost\n",
    "            \n",
    "            print(f\"ğŸ’° {module_name} call #{self.call_count}: ${cost:.5f}\")\n",
    "        else:\n",
    "            print(f\"ğŸ’° {module_name} call #{self.call_count}: No cost data\")\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print cost summary.\"\"\"\n",
    "        print(\"\\nğŸ“Š Cost Tracking Summary:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"Total calls: {self.call_count}\")\n",
    "        \n",
    "        if self.total_cost > 0:\n",
    "            print(f\"Total cost: ${self.total_cost:.5f}\")\n",
    "            if self.call_count > 0:\n",
    "                print(f\"Average cost per call: ${self.total_cost/self.call_count:.5f}\")\n",
    "        else:\n",
    "            print(\"Total cost: No cost data available\")\n",
    "        \n",
    "        if self.costs_by_module:\n",
    "            print(\"\\nCost by module:\")\n",
    "            for module, cost in self.costs_by_module.items():\n",
    "                if cost > 0:\n",
    "                    print(f\"  {module}: ${cost:.5f}\")\n",
    "\n",
    "# Register and use the custom callback\n",
    "cost_tracker = CostTracker()\n",
    "manager = CallbackManager()\n",
    "manager.register(cost_tracker)\n",
    "\n",
    "# Make some calls with cost tracking\n",
    "qa = Predict(SimpleQA, debug=True)  # Debug mode captures costs\n",
    "\n",
    "print(\"ğŸ”„ Making tracked calls...\")\n",
    "for q in [\"What is AI?\", \"Explain quantum computing\"]:\n",
    "    result = await qa(question=q)\n",
    "\n",
    "# Show summary\n",
    "cost_tracker.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Part 3: Persistence - Save & Load Your Work\n",
    "\n",
    "Being able to save configured modules and load them later is essential for production deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Created DocumentAnalysis module\n",
      "  Module type: ChainOfThought\n",
      "  Adapter: json\n",
      "  Has reasoning: True\n",
      "ğŸ’° ChainOfThought call #3: No cost data\n",
      "\n",
      "âœ… Module works! Summary: LogiLLM is a robust framework offering clean APIs ...\n",
      "\n",
      "ğŸ’¾ Now let's save this configured module...\n"
     ]
    }
   ],
   "source": [
    "# Define a signature with multiple fields\n",
    "class DocumentAnalysis(Signature):\n",
    "    \"\"\"Analyze documents for key information.\"\"\"\n",
    "    document: str = InputField(desc=\"Document to analyze\")\n",
    "    \n",
    "    summary: str = OutputField(desc=\"Brief summary\")\n",
    "    key_points: list[str] = OutputField(desc=\"Main points\")\n",
    "    sentiment: str = OutputField(desc=\"Overall sentiment\")\n",
    "    category: str = OutputField(desc=\"Document category\")\n",
    "\n",
    "# Create and configure a module\n",
    "analyzer = ChainOfThought(DocumentAnalysis, adapter=\"json\")\n",
    "\n",
    "print(\"ğŸ“ Created DocumentAnalysis module\")\n",
    "print(f\"  Module type: {type(analyzer).__name__}\")\n",
    "print(f\"  Adapter: {analyzer.adapter.format_type.value}\")\n",
    "print(f\"  Has reasoning: {'reasoning' in analyzer.signature.output_fields}\")\n",
    "\n",
    "# Test the module before saving\n",
    "test_doc = \"LogiLLM is a powerful framework for building LLM applications. It provides clean APIs and reliable parsing.\"\n",
    "result = await analyzer(document=test_doc)\n",
    "\n",
    "print(f\"\\nâœ… Module works! Summary: {result.outputs['summary'][:50]}...\")\n",
    "\n",
    "print(\"\\nğŸ’¾ Now let's save this configured module...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“¦ Method 1: Pickle - Simple and Reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saved module to: /tmp/logillm_notebook_c36fth53/analyzer.pkl\n",
      "ğŸ“Š File size: 3,302 bytes\n",
      "\n",
      "âœ… Loaded module successfully!\n",
      "  Type: ChainOfThought\n",
      "  Has signature: True\n",
      "  Has adapter: True\n",
      "ğŸ’° ChainOfThought call #4: No cost data\n",
      "\n",
      "ğŸ¯ Loaded module works!\n",
      "  Summary: Python is a popular and versatile programming lang...\n",
      "  Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the module with pickle\n",
    "save_path = TEMP_DIR / \"analyzer.pkl\"\n",
    "\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(analyzer, f)\n",
    "\n",
    "print(f\"ğŸ’¾ Saved module to: {save_path}\")\n",
    "print(f\"ğŸ“Š File size: {save_path.stat().st_size:,} bytes\")\n",
    "\n",
    "# Load the module back\n",
    "with open(save_path, 'rb') as f:\n",
    "    loaded_analyzer = pickle.load(f)\n",
    "\n",
    "print(f\"\\nâœ… Loaded module successfully!\")\n",
    "print(f\"  Type: {type(loaded_analyzer).__name__}\")\n",
    "print(f\"  Has signature: {hasattr(loaded_analyzer, 'signature')}\")\n",
    "print(f\"  Has adapter: {hasattr(loaded_analyzer, 'adapter')}\")\n",
    "\n",
    "# Test that the loaded module works\n",
    "test_doc2 = \"Python is a versatile programming language loved by developers worldwide.\"\n",
    "result2 = await loaded_analyzer(document=test_doc2)\n",
    "\n",
    "print(f\"\\nğŸ¯ Loaded module works!\")\n",
    "print(f\"  Summary: {result2.outputs['summary'][:50]}...\")\n",
    "print(f\"  Sentiment: {result2.outputs['sentiment']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ—‚ï¸ Managing Multiple Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved module 'document_analyzer'\n",
      "âœ… Saved module 'simple_qa'\n",
      "âœ… Saved module 'summarizer'\n",
      "\n",
      "ğŸ“š Module Registry:\n",
      "==================================================\n",
      "\n",
      "ğŸ“¦ document_analyzer\n",
      "   Type: ChainOfThought\n",
      "   Description: Analyzes documents with ChainOfThought reasoning\n",
      "\n",
      "ğŸ“¦ simple_qa\n",
      "   Type: Predict\n",
      "   Description: Simple question-answering module\n",
      "\n",
      "ğŸ“¦ summarizer\n",
      "   Type: Predict\n",
      "   Description: Text summarization module\n",
      "\n",
      "ğŸ’¾ Registry location: /tmp/logillm_notebook_c36fth53/modules\n"
     ]
    }
   ],
   "source": [
    "# Create a module registry for managing multiple modules\n",
    "class ModuleRegistry:\n",
    "    \"\"\"Simple registry for saving and loading multiple modules.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: Path):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(exist_ok=True)\n",
    "        self.registry_file = self.base_path / \"registry.json\"\n",
    "        self.registry = self._load_registry()\n",
    "    \n",
    "    def _load_registry(self) -> dict:\n",
    "        \"\"\"Load or create registry.\"\"\"\n",
    "        if self.registry_file.exists():\n",
    "            with open(self.registry_file) as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_registry(self):\n",
    "        \"\"\"Save registry to disk.\"\"\"\n",
    "        with open(self.registry_file, 'w') as f:\n",
    "            json.dump(self.registry, f, indent=2)\n",
    "    \n",
    "    def save_module(self, name: str, module, description: str = \"\"):\n",
    "        \"\"\"Save a module to the registry.\"\"\"\n",
    "        module_path = self.base_path / f\"{name}.pkl\"\n",
    "        \n",
    "        # Save module\n",
    "        with open(module_path, 'wb') as f:\n",
    "            pickle.dump(module, f)\n",
    "        \n",
    "        # Update registry\n",
    "        self.registry[name] = {\n",
    "            \"path\": str(module_path),\n",
    "            \"type\": type(module).__name__,\n",
    "            \"description\": description,\n",
    "            \"saved_at\": str(Path(tempfile.gettempdir()).parent)  # timestamp proxy\n",
    "        }\n",
    "        self._save_registry()\n",
    "        \n",
    "        print(f\"âœ… Saved module '{name}'\")\n",
    "        return module_path\n",
    "    \n",
    "    def load_module(self, name: str):\n",
    "        \"\"\"Load a module from the registry.\"\"\"\n",
    "        if name not in self.registry:\n",
    "            raise ValueError(f\"Module '{name}' not found in registry\")\n",
    "        \n",
    "        module_path = self.registry[name][\"path\"]\n",
    "        with open(module_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    def list_modules(self):\n",
    "        \"\"\"List all saved modules.\"\"\"\n",
    "        return self.registry\n",
    "\n",
    "# Create a registry\n",
    "registry = ModuleRegistry(TEMP_DIR / \"modules\")\n",
    "\n",
    "# Save multiple configured modules\n",
    "registry.save_module(\n",
    "    \"document_analyzer\",\n",
    "    analyzer,\n",
    "    \"Analyzes documents with ChainOfThought reasoning\"\n",
    ")\n",
    "\n",
    "registry.save_module(\n",
    "    \"simple_qa\",\n",
    "    qa,\n",
    "    \"Simple question-answering module\"\n",
    ")\n",
    "\n",
    "# Create and save another module\n",
    "summarizer = Predict(SimpleQA)  # Reusing SimpleQA for demo\n",
    "registry.save_module(\n",
    "    \"summarizer\",\n",
    "    summarizer,\n",
    "    \"Text summarization module\"\n",
    ")\n",
    "\n",
    "# List all saved modules\n",
    "print(\"\\nğŸ“š Module Registry:\")\n",
    "print(\"=\" * 50)\n",
    "for name, info in registry.list_modules().items():\n",
    "    print(f\"\\nğŸ“¦ {name}\")\n",
    "    print(f\"   Type: {info['type']}\")\n",
    "    print(f\"   Description: {info['description']}\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Registry location: {registry.base_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading modules from registry...\n",
      "\n",
      "âœ… Loaded 'document_analyzer'\n",
      "   Type: ChainOfThought\n",
      "ğŸ’° ChainOfThought call #5: No cost data\n",
      "\n",
      "ğŸ“ Analysis Results:\n",
      "   Summary: The document highlights how AI is transforming industries by enhancing efficiency and decision-making while emphasizing the need for ethical considerations.\n",
      "   Sentiment: Positive with caution\n",
      "   Key Points: 3 found\n",
      "ğŸ’° Predict call #5: No cost data\n",
      "\n",
      "ğŸ¯ QA Module Result: The capital of Japan is Tokyo.\n"
     ]
    }
   ],
   "source": [
    "# Load modules from registry\n",
    "print(\"ğŸ”„ Loading modules from registry...\\n\")\n",
    "\n",
    "# Load the document analyzer\n",
    "loaded_doc_analyzer = registry.load_module(\"document_analyzer\")\n",
    "print(f\"âœ… Loaded 'document_analyzer'\")\n",
    "print(f\"   Type: {type(loaded_doc_analyzer).__name__}\")\n",
    "\n",
    "# Use the loaded module\n",
    "test_doc3 = \"\"\"\n",
    "Artificial Intelligence is transforming industries worldwide.\n",
    "From healthcare to finance, AI systems are improving efficiency and decision-making.\n",
    "However, ethical considerations remain paramount.\n",
    "\"\"\"\n",
    "\n",
    "result3 = await loaded_doc_analyzer(document=test_doc3)\n",
    "\n",
    "print(f\"\\nğŸ“ Analysis Results:\")\n",
    "print(f\"   Summary: {result3.outputs['summary']}\")\n",
    "print(f\"   Sentiment: {result3.outputs['sentiment']}\")\n",
    "print(f\"   Key Points: {len(result3.outputs.get('key_points', []))} found\")\n",
    "\n",
    "# Load and use another module\n",
    "loaded_qa = registry.load_module(\"simple_qa\")\n",
    "qa_result = await loaded_qa(question=\"What is the capital of Japan?\")\n",
    "print(f\"\\nğŸ¯ QA Module Result: {qa_result.outputs['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Part 4: Production Patterns\n",
    "\n",
    "Let's combine debugging, logging, and persistence into production-ready patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Logging to: /tmp/logillm_notebook_c36fth53/production/email_processor.jsonl\n",
      "ğŸš€ Production Module Ready!\n",
      "   Name: email_processor\n",
      "   Base Path: /tmp/logillm_notebook_c36fth53/production\n",
      "ğŸ’° ChainOfThought call #6: No cost data\n",
      "ğŸ“Š Metric: success=1 (3.36s)\n",
      "\n",
      "ğŸ“§ Email Analysis:\n",
      "   Category: Project Update\n",
      "   Sentiment: Positive\n",
      "ğŸ’¾ Saved to: /tmp/logillm_notebook_c36fth53/production/email_processor.pkl\n",
      "\n",
      "ğŸ“Š Module Statistics:\n",
      "   total_calls: 1\n",
      "   total_tokens: 336\n",
      "   total_cost: 0\n",
      "   errors: 0\n"
     ]
    }
   ],
   "source": [
    "# Production-ready module with full observability\n",
    "class ProductionModule:\n",
    "    \"\"\"A production-ready wrapper for LogiLLM modules.\"\"\"\n",
    "    \n",
    "    def __init__(self, module_name: str, signature, config: dict = None):\n",
    "        self.module_name = module_name\n",
    "        self.config = config or {}\n",
    "        \n",
    "        # Setup paths\n",
    "        self.base_path = Path(self.config.get('base_path', './production'))\n",
    "        self.base_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Setup logging\n",
    "        self.log_file = self.base_path / f\"{module_name}.jsonl\"\n",
    "        self.callback_id = None\n",
    "        \n",
    "        # Create module with configuration\n",
    "        module_class = self.config.get('module_class', Predict)\n",
    "        self.module = module_class(\n",
    "            signature,\n",
    "            adapter=self.config.get('adapter', 'json'),\n",
    "            debug=self.config.get('debug', False)\n",
    "        )\n",
    "        \n",
    "        # Setup monitoring\n",
    "        self._setup_monitoring()\n",
    "        \n",
    "    def _setup_monitoring(self):\n",
    "        \"\"\"Setup logging and callbacks.\"\"\"\n",
    "        # Register JSONL logger\n",
    "        self.callback_id = register_jsonl_logger(\n",
    "            str(self.log_file),\n",
    "            include_module_events=True,\n",
    "            include_provider_events=True\n",
    "        )\n",
    "        print(f\"ğŸ“ Logging to: {self.log_file}\")\n",
    "        \n",
    "    async def __call__(self, **kwargs):\n",
    "        \"\"\"Execute module with monitoring.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Execute module\n",
    "            result = await self.module(**kwargs)\n",
    "            \n",
    "            # Log success metrics\n",
    "            duration = time.time() - start_time\n",
    "            self._log_metric(\"success\", 1, duration)\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log failure\n",
    "            duration = time.time() - start_time\n",
    "            self._log_metric(\"failure\", 1, duration)\n",
    "            print(f\"âŒ Error in {self.module_name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _log_metric(self, metric_type: str, value: float, duration: float):\n",
    "        \"\"\"Log custom metrics.\"\"\"\n",
    "        metric = {\n",
    "            \"timestamp\": time.time(),\n",
    "            \"module\": self.module_name,\n",
    "            \"type\": metric_type,\n",
    "            \"value\": value,\n",
    "            \"duration\": duration\n",
    "        }\n",
    "        # In production, send to metrics system\n",
    "        print(f\"ğŸ“Š Metric: {metric_type}={value} ({duration:.2f}s)\")\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save module to disk.\"\"\"\n",
    "        save_path = self.base_path / f\"{self.module_name}.pkl\"\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(self.module, f)\n",
    "        print(f\"ğŸ’¾ Saved to: {save_path}\")\n",
    "        return save_path\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"Load module from disk.\"\"\"\n",
    "        load_path = self.base_path / f\"{self.module_name}.pkl\"\n",
    "        if load_path.exists():\n",
    "            with open(load_path, 'rb') as f:\n",
    "                self.module = pickle.load(f)\n",
    "            print(f\"âœ… Loaded from: {load_path}\")\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get module statistics from logs.\"\"\"\n",
    "        if not self.log_file.exists():\n",
    "            return {}\n",
    "        \n",
    "        stats = {\n",
    "            \"total_calls\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"total_cost\": 0,\n",
    "            \"errors\": 0\n",
    "        }\n",
    "        \n",
    "        with open(self.log_file) as f:\n",
    "            for line in f:\n",
    "                event = json.loads(line)\n",
    "                if event.get(\"event_type\") == \"module_end\":\n",
    "                    stats[\"total_calls\"] += 1\n",
    "                elif event.get(\"event_type\") == \"provider_response\":\n",
    "                    usage = event.get(\"usage\", {})\n",
    "                    stats[\"total_tokens\"] += usage.get(\"total_tokens\", 0)\n",
    "                    stats[\"total_cost\"] += event.get(\"cost\", 0)\n",
    "                elif event.get(\"event_type\") == \"error\":\n",
    "                    stats[\"errors\"] += 1\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Create a production module\n",
    "prod_config = {\n",
    "    'module_class': ChainOfThought,\n",
    "    'adapter': 'json',\n",
    "    'debug': True,  # Enable debug in development\n",
    "    'base_path': TEMP_DIR / 'production'\n",
    "}\n",
    "\n",
    "prod_module = ProductionModule(\n",
    "    \"email_processor\",\n",
    "    DocumentAnalysis,\n",
    "    config=prod_config\n",
    ")\n",
    "\n",
    "print(\"ğŸš€ Production Module Ready!\")\n",
    "print(f\"   Name: {prod_module.module_name}\")\n",
    "print(f\"   Base Path: {prod_module.base_path}\")\n",
    "\n",
    "# Use the production module\n",
    "email_text = \"\"\"\n",
    "Subject: Project Update\n",
    "\n",
    "The development team has completed the API integration ahead of schedule.\n",
    "All tests are passing and we're ready for deployment next week.\n",
    "Please review the documentation and provide feedback by Friday.\n",
    "\n",
    "Best regards,\n",
    "Development Team\n",
    "\"\"\"\n",
    "\n",
    "result = await prod_module(document=email_text)\n",
    "\n",
    "print(f\"\\nğŸ“§ Email Analysis:\")\n",
    "print(f\"   Category: {result.outputs['category']}\")\n",
    "print(f\"   Sentiment: {result.outputs['sentiment']}\")\n",
    "\n",
    "# Save the module\n",
    "prod_module.save()\n",
    "\n",
    "# Get statistics\n",
    "stats = prod_module.get_stats()\n",
    "print(f\"\\nğŸ“Š Module Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Key Takeaways\n",
    "\n",
    "<div style=\"background: #e8f5e9; padding: 25px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h3 style=\"margin-top: 0;\">ğŸ” Debugging Best Practices</h3>\n",
    "    <ul style=\"margin: 15px 0;\">\n",
    "        <li>Enable <code>debug=True</code> during development</li>\n",
    "        <li>Access complete request/response data via <code>result.request</code> and <code>result.response</code></li>\n",
    "        <li>Monitor token usage and costs with <code>result.response['usage']</code></li>\n",
    "        <li>Use environment variable <code>LOGILLM_DEBUG=1</code> for global debugging</li>\n",
    "    </ul>    \n",
    "    <h3 style=\"margin-top: 20px;\">ğŸ“Š Logging & Monitoring</h3>\n",
    "    <ul style=\"margin: 15px 0;\">\n",
    "        <li>Use JSONL logging for structured, parseable logs</li>\n",
    "        <li>Create custom callbacks for specific monitoring needs</li>\n",
    "        <li>Track costs, latency, and errors systematically</li>\n",
    "        <li>Analyze logs to optimize performance and costs</li>\n",
    "    </ul>    \n",
    "    <h3 style=\"margin-top: 20px;\">ğŸ’¾ Persistence Strategies</h3>\n",
    "    <ul style=\"margin: 15px 0;\">\n",
    "        <li>Use pickle for simple, reliable module saving</li>\n",
    "        <li>Create registries for managing multiple modules</li>\n",
    "        <li>Save configured modules for consistent behavior</li>\n",
    "        <li>Version your saved modules for reproducibility</li>\n",
    "    </ul>    \n",
    "    <h3 style=\"margin-top: 20px;\">ğŸ—ï¸ Production Patterns</h3>\n",
    "    <ul style=\"margin: 15px 0;\">\n",
    "        <li>Combine debugging, logging, and persistence</li>\n",
    "        <li>Create wrapper classes for production modules</li>\n",
    "        <li>Implement comprehensive error handling</li>\n",
    "        <li>Monitor performance and costs continuously</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ® Interactive Exercise\n",
    "\n",
    "Build your own monitored module with debugging, logging, and persistence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“§ Analyzing Support Tickets:\n",
      "============================================================\n",
      "ğŸ’° ChainOfThought call #7: No cost data\n",
      "ğŸš¨ URGENT: Customer Unknown - billing\n",
      "\n",
      "ğŸ« Ticket from CUST-001:\n",
      "   Urgency: high\n",
      "   Category: billing\n",
      "   Escalate: False\n",
      "   Response: We apologize for the inconvenience caused by the duplicate charge. We are reviewing your account and...\n",
      "ğŸ’° ChainOfThought call #8: No cost data\n",
      "\n",
      "ğŸ« Ticket from CUST-002:\n",
      "   Urgency: low\n",
      "   Category: general\n",
      "   Escalate: False\n",
      "   Response: Hello, thank you for reaching out. To update your email address in the system, please log into your ...\n",
      "ğŸ’° ChainOfThought call #9: No cost data\n",
      "ğŸš¨ URGENT: Customer Unknown - technical\n",
      "\n",
      "ğŸ« Ticket from CUST-003:\n",
      "   Urgency: high\n",
      "   Category: technical\n",
      "   Escalate: True\n",
      "   Response: We are very sorry to hear about the issues you're experiencing with the app crashing upon login and ...\n",
      "\n",
      "ğŸ’¾ Saved analyzer to: /tmp/logillm_notebook_c36fth53/ticket_analyzer.pkl\n",
      "\n",
      "ğŸš¨ Urgent tickets found: 2\n"
     ]
    }
   ],
   "source": [
    "# Exercise: Create a customer support ticket analyzer\n",
    "# with full debugging, logging, and persistence\n",
    "\n",
    "class SupportTicket(Signature):\n",
    "    \"\"\"Analyze customer support tickets.\"\"\"\n",
    "    ticket_text: str = InputField(desc=\"Support ticket content\")\n",
    "    customer_id: str = InputField(desc=\"Customer identifier\")\n",
    "    \n",
    "    urgency: str = OutputField(desc=\"high, medium, or low\")\n",
    "    category: str = OutputField(desc=\"billing, technical, general\")\n",
    "    sentiment: str = OutputField(desc=\"positive, negative, neutral\")\n",
    "    needs_escalation: bool = OutputField(desc=\"Requires manager attention\")\n",
    "    suggested_response: str = OutputField(desc=\"Draft response to customer\")\n",
    "\n",
    "# TODO: Your implementation here!\n",
    "# 1. Create a module with debug mode enabled\n",
    "# 2. Setup JSONL logging\n",
    "# 3. Create a custom callback to track urgent tickets\n",
    "# 4. Save the configured module\n",
    "# 5. Test with sample tickets\n",
    "\n",
    "# Your code here:\n",
    "ticket_analyzer = ChainOfThought(SupportTicket, adapter=\"json\", debug=True)\n",
    "\n",
    "# Setup logging\n",
    "ticket_log = TEMP_DIR / \"support_tickets.jsonl\"\n",
    "ticket_callback = register_jsonl_logger(str(ticket_log))\n",
    "\n",
    "# Custom callback for urgent tickets\n",
    "class UrgentTicketTracker(AbstractCallback):\n",
    "    def __init__(self):\n",
    "        self.urgent_tickets = []\n",
    "    \n",
    "    async def on_module_end(self, event):\n",
    "        if hasattr(event, 'prediction') and event.prediction:\n",
    "            outputs = event.prediction.outputs\n",
    "            inputs = event.prediction.inputs if hasattr(event.prediction, 'inputs') else {}\n",
    "            \n",
    "            if outputs.get('urgency') == 'high' or outputs.get('needs_escalation'):\n",
    "                customer_id = inputs.get('customer_id', 'Unknown')\n",
    "                self.urgent_tickets.append({\n",
    "                    'time': time.time(),\n",
    "                    'customer': customer_id,\n",
    "                    'urgency': outputs.get('urgency'),\n",
    "                    'category': outputs.get('category')\n",
    "                })\n",
    "                print(f\"ğŸš¨ URGENT: Customer {customer_id} - {outputs.get('category')}\")\n",
    "\n",
    "urgent_tracker = UrgentTicketTracker()\n",
    "CallbackManager().register(urgent_tracker)\n",
    "\n",
    "# Test tickets\n",
    "test_tickets = [\n",
    "    (\"My account was charged twice! This is unacceptable! I want a refund NOW!\", \"CUST-001\"),\n",
    "    (\"How do I update my email address in the system?\", \"CUST-002\"),\n",
    "    (\"The app crashes every time I try to login. I've lost important data!\", \"CUST-003\")\n",
    "]\n",
    "\n",
    "print(\"ğŸ“§ Analyzing Support Tickets:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for ticket_text, customer_id in test_tickets:\n",
    "    result = await ticket_analyzer(\n",
    "        ticket_text=ticket_text,\n",
    "        customer_id=customer_id\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ« Ticket from {customer_id}:\")\n",
    "    print(f\"   Urgency: {result.outputs['urgency']}\")\n",
    "    print(f\"   Category: {result.outputs['category']}\")\n",
    "    print(f\"   Escalate: {result.outputs['needs_escalation']}\")\n",
    "    print(f\"   Response: {result.outputs['suggested_response'][:100]}...\")\n",
    "\n",
    "# Save the analyzer\n",
    "save_path = TEMP_DIR / \"ticket_analyzer.pkl\"\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(ticket_analyzer, f)\n",
    "\n",
    "print(f\"\\nğŸ’¾ Saved analyzer to: {save_path}\")\n",
    "print(f\"\\nğŸš¨ Urgent tickets found: {len(urgent_tracker.urgent_tickets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ Progress Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Notebook Progress: 10/10 sections (100%)\n",
      "\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n",
      "ğŸ‰ Excellent! You've mastered Debugging, Logging & Persistence!\n",
      "\n",
      "ğŸ”‘ Key skills acquired:\n",
      "  â€¢ Debug LLM interactions effectively\n",
      "  â€¢ Implement comprehensive logging\n",
      "  â€¢ Create custom monitoring callbacks\n",
      "  â€¢ Save and load configured modules\n",
      "  â€¢ Build production-ready systems\n",
      "\n",
      "Ready for Notebook 5: Optimization!\n"
     ]
    }
   ],
   "source": [
    "# Progress tracker\n",
    "completed = {\n",
    "    \"debugging_basics\": True,\n",
    "    \"debug_inspection\": True,\n",
    "    \"debug_methods\": True,\n",
    "    \"jsonl_logging\": True,\n",
    "    \"custom_callbacks\": True,\n",
    "    \"saving_modules\": True,\n",
    "    \"loading_modules\": True,\n",
    "    \"module_registry\": True,\n",
    "    \"production_patterns\": True,\n",
    "    \"exercise\": True\n",
    "}\n",
    "\n",
    "total = len(completed)\n",
    "done = sum(completed.values())\n",
    "percentage = (done / total) * 100 if total > 0 else 0\n",
    "\n",
    "print(f\"ğŸ“Š Notebook Progress: {done}/{total} sections ({percentage:.0f}%)\")\n",
    "print(\"\\n\" + \"â–ˆ\" * int(percentage // 5) + \"â–‘\" * (20 - int(percentage // 5)))\n",
    "\n",
    "if percentage == 100:\n",
    "    print(\"\\nğŸ‰ Excellent! You've mastered Debugging, Logging & Persistence!\")\n",
    "    print(\"\\nğŸ”‘ Key skills acquired:\")\n",
    "    print(\"  â€¢ Debug LLM interactions effectively\")\n",
    "    print(\"  â€¢ Implement comprehensive logging\")\n",
    "    print(\"  â€¢ Create custom monitoring callbacks\")\n",
    "    print(\"  â€¢ Save and load configured modules\")\n",
    "    print(\"  â€¢ Build production-ready systems\")\n",
    "    print(\"\\nReady for Notebook 5: Optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; margin-top: 40px; padding: 20px; background: #f5f5f5; border-radius: 10px;\">\n",
    "    <a href=\"03_modules_adapters.ipynb\" style=\"text-decoration: none; padding: 10px 20px; background: white; border-radius: 5px; border: 1px solid #ddd;\">â† Notebook 3</a>\n",
    "    <div style=\"text-align: center;\">\n",
    "        <strong>Congratulations! You're ready for production! ğŸš€</strong>\n",
    "    </div>\n",
    "    <a href=\"05_optimization.ipynb\" style=\"text-decoration: none; padding: 10px 20px; background: #667eea; color: white; border-radius: 5px;\">Continue to Notebook 5 â†’</a>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
